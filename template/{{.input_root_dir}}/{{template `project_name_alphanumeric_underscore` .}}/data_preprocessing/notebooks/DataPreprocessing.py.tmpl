# Databricks notebook source
##################################################################################
# Data Preprocessing Notebook
#
# This notebook shows an example of a Data Preprocessing pipeline using Unity Catalog.
# It is configured and can be executed as the tasks in the PreprocessRawData workflow defined under
# ``{{template `project_name_alphanumeric_underscore` .}}/resources/data-preprocessing-workflow-resource.yml``
#
# Parameters:
# * uc_catalog (required)                     - Name of the Unity Catalog 
# * database_name (required)                  - Name of the database inside Unity Catalog 
# * raw_data_table_name (required)            - Name of the raw data table inside UC database
# * preprocessed_data_table_name (required)   - Name of the preprocessed data table inside UC database
# * max_chunk_size (optional)                 - Maximum chunk size
# * min_chunk_size (optional)                 - Minimum chunk size
# * chunk_overlap (optional)                  - Chunk overlap between chunks 
#  
##################################################################################


%pip install --quiet transformers==4.30.2 langchain==0.2.1
dbutils.library.restartPython()

# COMMAND ----------


# List of input args needed to run this notebook as a job.
# Provide them via DB widgets or notebook arguments.
#
# A Unity Catalog location containing the input data.
dbutils.widgets.text(
    "uc_catalog",
    "llmops_stacks",
    label="Name of Unity Catalog",
)
# Name of database inside Unity Catalog.
dbutils.widgets.text(
    "database_name",
    "rag_chatbot",
    label="Name of database inside Unity Catalog",
)
# Name of input table inside database of Unity Catalog.
dbutils.widgets.text(
    "raw_data_table_name",
    "raw_documentation",
    label="Raw data Table Name",
)
# Name of output table inside database of Unity Catalog.
dbutils.widgets.text(
    "preprocessed_data_table_name",
    "databricks_documentation",
    label="Preprocessed data Table Name",
)
# Maximum chunk size.
dbutils.widgets.text("max_chunk_size", "500", label="Maximum chunk size")
# Minimum chunk size.
dbutils.widgets.text("min_chunk_size", "20", label="Minimum chunk size")
# Chunk overlap.
dbutils.widgets.text("chunk_overlap", "50", label="Chunk overlap")

# COMMAND ----------

import os
notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())
%cd $notebook_path
%cd ../preprocessing

# COMMAND ----------

# DBTITLE 1,Define input and output variables
uc_catalog = dbutils.widgets.get("uc_catalog")
database_name = dbutils.widgets.get("database_name")
raw_data_table_name = dbutils.widgets.get("raw_data_table_name")
preprocessed_data_table_name = dbutils.widgets.get("preprocessed_data_table_name")

assert uc_catalog != "", "uc_catalog notebook parameter must be specified"
assert database_name != "", "database_name notebook parameter must be specified"
assert raw_data_table_name != "", "raw_data_table_name notebook parameter must be specified"
assert preprocessed_data_table_name != "", "preprocessed_data_table_name notebook parameter must be specified"

max_chunk_size = int(dbutils.widgets.get("max_chunk_size"))
min_chunk_size = int(dbutils.widgets.get("min_chunk_size"))
chunk_overlap = int(dbutils.widgets.get("chunk_overlap"))

# COMMAND ----------

# DBTITLE 1, Use the catalog and database specified in the notebook parameters
spark.sql(f"""USE `{uc_catalog}`.`{database_name}`""")   

# COMMAND ----------

# DBTITLE 1, Create output preprocessed data table
if not spark.catalog.tableExists(f"{preprocessed_data_table_name}") or spark.table(f"{preprocessed_data_table_name}").isEmpty():
  spark.sql(f"""
  CREATE TABLE IF NOT EXISTS {preprocessed_data_table_name} (
    id BIGINT GENERATED ALWAYS AS IDENTITY,
    url STRING,
    content STRING
  )
  TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')
  """)


# COMMAND ----------

# DBTITLE 1,Create a user-defined function (UDF) to chunk all our documents with spark.
import pandas as pd
from functools import partial
from importlib import import_module
from pyspark.sql.functions import pandas_udf
from create_chunk import split_html_on_h2


@pandas_udf("array<string>")
def parse_and_split(
    docs: pd.Series
) -> pd.Series:
    """Parse and split html content into chunks.

    :param docs: Input documents
    :return: List of chunked text for each input document
    """
    # TODO: Apply partial to pass chunk, max and min sizes
    return docs.apply(split_html_on_h2)

# COMMAND ----------

from pyspark.sql import functions as F


# DBTITLE 1,Perform data preprocessing.
(spark.table(raw_data_table_name)
      .filter('text is not null')
      .withColumn('content', F.explode(parse_and_split('text')))
      .drop("text")
      .write.mode('overwrite').saveAsTable(preprocessed_data_table_name))

# COMMAND ----------

dbutils.notebook.exit(0)
