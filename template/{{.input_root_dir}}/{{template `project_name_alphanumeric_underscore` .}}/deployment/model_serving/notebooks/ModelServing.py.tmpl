# Databricks notebook source
##################################################################################
# Helper notebook to serve the model on an endpoint. This notebook is run
# after the ModelDeployment.py notebook as part of a multi-task job, in order to serve the model
# on an endpoint stage after transitioning the latest version.
#
# This notebook has the following parameters:

#  * model_uri (required)  - URI of the model to deploy. Must be in the format "models:/<name>/<version-id>", as described in
#                            https://www.mlflow.org/docs/latest/model-registry.html#fetching-an-mlflow-model-from-the-model-registry
#                            This parameter is read as a task value
#                            ({{ template `generate_doc_link` (map (pair "cloud" .input_cloud) (pair "path" "dev-tools/databricks-utils.html")) }}),
#                            rather than as a notebook widget. That is, we assume a preceding task (the Train.py
#                            notebook) has set a task value with key "model_uri".
#  * scale_to_zero (required)  - Specify if the endpoint should scale to zero when not in use.
#  * workload_size (required)  - Specify  the size of the compute scale out that corresponds with the number of requests this served 
#                                model can process at the same time. This number should be roughly equal to QPS x model run time.
##################################################################################

# List of input args needed to run the notebook as a job.
# Provide them via DB widgets or notebook arguments.
#
# Name of the current environment
dbutils.widgets.dropdown("scale_to_zero", "True", ["True", "False"], "Scale to zero")
dbutils.widgets.dropdown("workload_size", "Small", ["Small", "Medium", "Large"], "Workload Size")

# COMMAND ----------

# MAGIC %load_ext autoreload
# MAGIC %autoreload 2

# COMMAND ----------

import os
notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())
%cd $notebook_path

# COMMAND ----------

# MAGIC %pip install -r ../../requirements.txt

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

import os
notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())
%cd $notebook_path
%cd ../

# COMMAND ----------

model_uri = dbutils.jobs.taskValues.get("ModelDeployment", "model_uri", debugValue="")
assert model_uri != "", "model_uri notebook parameter must be specified"

# COMMAND ----------

scale_to_zero = bool(dbutils.widgets.get("scale_to_zero"))
workload_size = dbutils.widgets.get("workload_size")

# COMMAND ----------

from databricks.sdk.service.serving import ServedModelInputWorkloadSize
ws_dict = {
    'Small' : ServedModelInputWorkloadSize.SMALL, 
    'Medium' : ServedModelInputWorkloadSize.MEDIUM, 
    'Large' : ServedModelInputWorkloadSize.LARGE
} 
workload_size = ws_dict[workload_size]

# COMMAND ----------

instructions_to_reviewer = f"""### Instructions for Testing the our Chatbot assistant

Your inputs are invaluable for the development team. By providing detailed feedback and corrections, you help us fix issues and improve the overall quality of the application. We rely on your expertise to identify any gaps or areas needing enhancement.

1. **Variety of Questions**:
   - Please try a wide range of questions that you anticipate the end users of the application will ask. This helps us ensure the application can handle the expected queries effectively.

2. **Feedback on Answers**:
   - After asking each question, use the feedback widgets provided to review the answer given by the application.
   - If you think the answer is incorrect or could be improved, please use "Edit Answer" to correct it. Your corrections will enable our team to refine the application's accuracy.

3. **Review of Returned Documents**:
   - Carefully review each document that the system returns in response to your question.
   - Use the thumbs up/down feature to indicate whether the document was relevant to the question asked. A thumbs up signifies relevance, while a thumbs down indicates the document was not useful.

Thank you for your time and effort in testing our assistant. Your contributions are essential to delivering a high-quality product to our end users."""

# COMMAND ----------

from databricks import agents

_, model_name, model_version = model_uri.split("/")

# Important when logging chain 
# mlflow.models.set_retriever_schema is required to:
# 1. Enable the RAG Studio Review App to properly display retrieved chunks
# 2. Enable evaluation suite to measure the retriever

# Deploy to enable the Review APP and create an API endpoint
# Note it deploys model on endpoint and enables inference table
deployment_info = agents.deploy(model_name=model_name, model_version=model_version, scale_to_zero=scale_to_zero, workload_size=workload_size)

# Add the user-facing instructions to the Review App
agents.set_review_instructions(model_name, instructions_to_reviewer)

# COMMAND ----------

from serving import wait_for_model_serving_endpoint_to_be_ready
wait_for_model_serving_endpoint_to_be_ready(deployment_info.endpoint_name)

# COMMAND ----------

#TODO grant your stakeholders permissions to use the Review App
user_list = ["firstname.lastname@company.com"]
# Set the permissions.
agents.set_permissions(model_name=model_name, users=user_list, permission_level=agents.PermissionLevel.CAN_QUERY)

print(f"Share this URL with your stakeholders: {deployment_info.review_app_url}")

# COMMAND ----------

#for deployment in agents.list_deployments():
#  if deployment.model_name == model_name:
#    print(f"Review App URL: {deployment.review_app_url}")   

# COMMAND ---------- 

#To query the endpoint we can do
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.serving import ChatMessage, ChatMessageRole

w = WorkspaceClient()
messages = []
messages.append(ChatMessage(content="What is a large language model?", role=ChatMessageRole.USER))
response = w.serving_endpoints.query(
    name=deployment_info.endpoint_name,
    messages=messages,
    temperature=1.0,
    stream=False,
)
print(response.choices[0].message.content)
